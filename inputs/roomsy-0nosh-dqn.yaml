name: rooms4-0nosh-dqn
comment: "4 rooms, continuous, Dueling DQN"

algorithm:
  name: multinav2
  repository: https://github.com/cipollone/multinav2/
  version: 0.2.7
  commit: null
  diff: null
  params:
    agent: DQN
    config:
      # Common
      num_workers: 1
      num_envs_per_worker: 1
      log_level: WARN
      num_gpus: 0.18
      batch_mode: complete_episodes
      gamma: 0.99
      framework: tf2
      eager_tracing: true
      eager_max_retraces: 12
      evaluation_interval: 5
      evaluation_duration: 5
      evaluation_duration_unit: episodes
      evaluation_num_workers: 0
      evaluation_config:
        explore: False
      # DQN parameters
      double_q: true
      dueling: true
      lr: 0.0001
      _disable_preprocessor_api: true
      model:
        custom_model: composite_fc
        layers: [64, 64, 64]
        shared_layers: 1
        activation: relu
        batch_norm: true
        # Todo: normalize
      exploration_config:
        type: EpsilonGreedy
        initial_epsilon: 1.0
        final_epsilon: 0.03
        epsilon_timesteps: tuned
      replay_buffer_config:
        capacity: 100000
        learning_starts: 10000
      shuffle_buffer_size: 10000
      train_batch_size: 64
      training_intensity: 256
    run:
      stop:
        timesteps_total: 400000
        episode_reward_mean: 1.0
      num_samples: 1
      checkpoint_freq: 50
      keep_checkpoints_num: 2
    tune:
      exploration_config:
        epsilon_timesteps: [400000]

environment:
  name: multinav2
  repository: https://github.com/cipollone/multinav2/
  version: same as algorithm
  commit: ''
  diff: ''
  params:
    env: rooms0
    reward_shift: 0.0
    initial_position: [3, 12]
    shaping: null
    shaping_gamma: 0.99
    return_invariant: true
    episode_time_limit: 200
    fail_p: 0.03
    render: false
    reward_per_step: 0.0
    reward_outside_grid: 0.0
    reward_duplicate_beep: 0.0
    angular_speed: 40
    acceleration: 0.2
    max_velocity: 0.6
    min_velocity: 0
    fluents: rooms
    rewards:
      - ldlf: "<(!yellow)*; yellow>end"
        reward: 1.0
    map: |-
      |###############|
      |#ggg#ppp#ppyyy#|
      |#ggg#p#p#p#yyy#|
      |#ggg#p#p#p#yyy#|
      |#ggggp#ppp#yyy#|
      |###bb######BBB#|
      |#bbbbbbbbb#BBB#|
      |#bbbbbbbbb#BBB#|
      |#bbbbbbbbbbooo#|
      |#bbbbbbbbbbooo#|
      |#bbbbbbbbbbooo#|
      |#bbbbbbbbbbrrr#|
      |#bbbbbbbbbbrrr#|
      |#bbbbbbbbb#rrr#|
      |###############|
evaluation:
  episodes: 0
  frequency: 0
n-runs: 1
output-base: outputs
run-command: "poetry run python -m multinav train"
  #run-command: "poetry-debugpy -m multinav train"
