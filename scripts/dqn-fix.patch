*** dqn.py	2022-08-06 15:28:08.543951025 +0200
--- rllib/agents/dqn/dqn.py	2022-08-06 14:21:41.254159530 +0200
*************** https://docs.ray.io/en/master/rllib-algo
*** 12,17 ****
--- 12,19 ----
  import logging
  from typing import List, Optional, Type
  
+ import numpy as np
+ 
  from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy
  from ray.rllib.agents.dqn.dqn_torch_policy import DQNTorchPolicy
  from ray.rllib.agents.dqn.simple_q import (
*************** def calculate_rr_weights(config: Trainer
*** 169,181 ****
      native_ratio = config["train_batch_size"] / (
          config["rollout_fragment_length"]
          * config["num_envs_per_worker"]
!         * config["num_workers"]
      )
  
      # Training intensity is specified in terms of
      # (steps_replayed / steps_sampled), so adjust for the native ratio.
!     weights = [1, config["training_intensity"] / native_ratio]
!     return weights
  
  
  class DQNTrainer(SimpleQTrainer):
--- 171,188 ----
      native_ratio = config["train_batch_size"] / (
          config["rollout_fragment_length"]
          * config["num_envs_per_worker"]
!         # Add one to workers because the local
!         # worker usually collects experiences as well, and we avoid division by zero.
!         * max(config["num_workers"], 1)
      )
  
      # Training intensity is specified in terms of
      # (steps_replayed / steps_sampled), so adjust for the native ratio.
!     sample_and_train_weight = config["training_intensity"] / native_ratio
!     if sample_and_train_weight < 1:
!         return [int(np.round(1 / sample_and_train_weight)), 1]
!     else:
!         return [1, int(np.round(sample_and_train_weight))]
  
  
  class DQNTrainer(SimpleQTrainer):
